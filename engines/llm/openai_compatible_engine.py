"""OpenAIåè®®å…¼å®¹çš„å¤§è¯­è¨€æ¨¡å‹å¼•æ“"""
from openai import AsyncOpenAI
from engines.base import BaseEngine
from loguru import logger
from typing import AsyncGenerator, List, Dict


class OpenAICompatibleEngine(BaseEngine):
    """OpenAIåè®®å…¼å®¹çš„LLMå¼•æ“ï¼ˆæ”¯æŒDeepSeekç­‰ï¼‰"""

    def __init__(
        self,
        api_base: str,
        api_key: str,
        model: str,
        system_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 500
    ):
        """
        åˆå§‹åŒ–LLMå¼•æ“

        Args:
            api_base: APIåŸºç¡€URL
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        """
        self.api_base = api_base
        self.api_key = api_key
        self.model = model
        self.system_prompt = system_prompt
        self.temperature = temperature
        self.max_tokens = max_tokens

        self.client = None
        self.conversation_history: List[Dict[str, str]] = []

    async def initialize(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.api_base
            )
            logger.info(f"LLMå¼•æ“å·²åˆå§‹åŒ–: {self.model}")
            logger.debug(f"API Base: {self.api_base}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–LLMå¼•æ“å¤±è´¥: {e}")
            raise

    async def chat_stream(self, message: str) -> AsyncGenerator[str, None]:
        """
        æµå¼å¯¹è¯

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯

        Yields:
            ç”Ÿæˆçš„token
        """
        if not self.client:
            raise RuntimeError("LLMå¼•æ“æœªåˆå§‹åŒ–")

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": message})

        # æ„é€ æ¶ˆæ¯
        messages = [
            {"role": "system", "content": self.system_prompt},
            *self.conversation_history
        ]

        try:
            # æµå¼è¯·æ±‚
            full_response = ""

            logger.debug(f"å‘èµ·LLMè¯·æ±‚: {message[:50]}...")

            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    full_response += token
                    yield token

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.conversation_history.append({"role": "assistant", "content": full_response})

            logger.info(f"ğŸ¤– LLMå›å¤: {full_response}")

        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            # ç”Ÿæˆé”™è¯¯æç¤º
            error_msg = "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚"
            self.conversation_history.append({"role": "assistant", "content": error_msg})
            yield error_msg

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
        logger.debug("å¯¹è¯å†å²å·²æ¸…ç©º")

    def get_history_length(self) -> int:
        """è·å–å¯¹è¯å†å²é•¿åº¦"""
        return len(self.conversation_history)

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.client:
            await self.client.close()
        self.conversation_history = []
        logger.info("LLMå¼•æ“å·²æ¸…ç†")
